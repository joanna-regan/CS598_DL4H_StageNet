{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joanna-regan/CS598_DL4H_StageNet/blob/main/JR_FinalProject_ExtraCredit_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U89NiEjviSj5"
      },
      "source": [
        "#Reproducibility Study of StageNet: \n",
        "**Stage-Aware Neural Networks for Health Risk Prediction**\n",
        "\n",
        "Original Athors:Junyi Gao, Cao Xiao, Yasha Wang, Wen Tang, Lucas M. Glass, and Jimeng Sun\n",
        "\n",
        "Reproduced by: Joanna Regan"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "eEtdkePLLvV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Reproducibility Summary:\n"
      ],
      "metadata": {
        "id": "N9Mw2YdyJc3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This report looks to reproduce the novel model StageNet, which is comprised of a stage-aware LSTM module and a stage-adaptive Convolution module. The goal of StageNet is to identify the points in time where health status changes rapidly (i.e., enters a new stage), and to dynamically learn information about the patterns within each stage to help make predictions about a patient's health risk.\n",
        "\n",
        "This report also looks to reproduce two reduced models of StageNet. StageNet-I replaces the stage-aware LSTM with a vanilla LSTM, and StageNet-II only uses the stage-aware LSTM and removes the stage-adaptive convolution module.\n",
        "\n",
        "We assess the claims that StageNet trained with MIMIC-III EHR data will achieve 10\\% higher AUPRC and min(Re, P+) than baseline models on decompensation risk prediction task, and that reduced models StageNet-I and StageNet-II will still achieve higher AUPRC, AUROC, and min(Re, P+) than all baseline models on decompensation risk prediction task.\n",
        "\n",
        "We found that we were unable to produce results as strong as those reported by the original work, though our results are in the right ballpark. The current study was limited to only use a small subset of data to train models, which likely contributed to the lower performance metrics."
      ],
      "metadata": {
        "id": "0JiWLD7PLxDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dBql8JAQL7vG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Statistics\n",
        "\n"
      ],
      "metadata": {
        "id": "YJXbZckNJjMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the dataset is not provided, but should be obtained from PhysioNet at:\n",
        "\n",
        "https://physionet.org/content/mimiciii/1.4/\n",
        "\n",
        "Data must then be preprocessed for the decompensation task according to:\n",
        "\n",
        "https://github.com/YerevaNN/mimic3-benchmarks/\n",
        "\n",
        "Final data samples should be added to /data/train_subdivided and /data/test to reproduce the results shown below.\n",
        "\n"
      ],
      "metadata": {
        "id": "u3NFem-0Jjln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first instantiate some parameters for our dataloaders and set our arguments:"
      ],
      "metadata": {
        "id": "eQCXGaSZNku1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "e5QXvLC2neIs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "import imp\n",
        "import re\n",
        "import pickle\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from time import perf_counter\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "\n",
        "RANDOM_SEED = 12345\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils import data\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed(RANDOM_SEED)\n",
        "torch.backends.cudnn.deterministic=True\n",
        "\n",
        "from utils import utils\n",
        "from utils.readers import DecompensationReader\n",
        "from utils.preprocessing import Discretizer, Normalizer\n",
        "from utils import metrics\n",
        "from utils import common_utils\n",
        "from model import StageNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s53gcZ54UiW"
      },
      "source": [
        "Confirm we're using GPU runtime:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZLkMbQ84W7X",
        "outputId": "56011d87-c219-463b-fc70-62e60305e26c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "available device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() == True else 'cpu')\n",
        "print(\"available device: {}\".format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F27WN3EIib-H"
      },
      "source": [
        "Define and load arguments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o3h_E519oQKH"
      },
      "outputs": [],
      "source": [
        "def parse_arguments(parser):\n",
        "    parser.add_argument('--test_mode', type=int, default=0, help='Test SA-CRNN on MIMIC-III dataset')\n",
        "    parser.add_argument('--data_path', type=str, metavar='<data_path>', help='The path to the MIMIC-III data directory')\n",
        "    parser.add_argument('--file_name', type=str, metavar='<data_path>', help='File name to save model')\n",
        "    parser.add_argument('--small_part', type=int, default=0, help='Use part of training data')\n",
        "    parser.add_argument('--batch_size', type=int, default=128, help='Training batch size')\n",
        "    parser.add_argument('--epochs', type=int, default=50, help='Training epochs')\n",
        "    parser.add_argument('--lr', type=float, default=0.001, help='Learing rate')\n",
        "\n",
        "    parser.add_argument('--input_dim', type=int, default=76, help='Dimension of visit record data')\n",
        "    parser.add_argument('--rnn_dim', type=int, default=384, help='Dimension of hidden units in RNN')\n",
        "    parser.add_argument('--output_dim', type=int, default=1, help='Dimension of prediction target')\n",
        "    parser.add_argument('--dropout_rate', type=float, default=0.5, help='Dropout rate')\n",
        "    parser.add_argument('--dropconnect_rate', type=float, default=0.5, help='Dropout rate in RNN')\n",
        "    parser.add_argument('--dropres_rate', type=float, default=0.3, help='Dropout rate in residue connection')\n",
        "    parser.add_argument('--K', type=int, default=10, help='Value of hyper-parameter K')\n",
        "    parser.add_argument('--chunk_level', type=int, default=3, help='Value of hyper-parameter K')\n",
        "\n",
        "    parser.add_argument('-f')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parse_arguments(parser)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we define our personal arguments. We use a small sample of data for illustrative purposes:"
      ],
      "metadata": {
        "id": "lM5JpE5PN_dE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#JR add in my paths:\n",
        "args.data_path = './data/'\n",
        "args.file_name = 'trained_model'\n",
        "args.epochs = 5\n",
        "args.small_part = 1000"
      ],
      "metadata": {
        "id": "b0px1gEPOITR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDlTZcv0ihVt"
      },
      "source": [
        "We now load the train, validation, and test sets data and report some statistics.\n",
        "\n",
        "The next cell should only take a few minutes to run. If running for over 5 minutes, recommend stopping the cell execution and rerunning.\n",
        "\n",
        "The following few cells in this section can be run for more statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A1-2B9Uq1qZ",
        "outputId": "8b75e0a6-0d0c-477c-813f-d05557de6fde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing data sets ... \n",
            "Generating data...\n",
            "Generating data...\n",
            "Generating data...\n",
            "Time to load train data: 0:00:20.314760\n",
            "Time to load validation data: 0:00:01.474405\n",
            "Time to load test data: 0:00:02.427864\n",
            "Size of training set: 1000\n",
            "Size of validation set: 112\n",
            "Size of test set: 278\n"
          ]
        }
      ],
      "source": [
        "print('Preparing data sets ... ')\n",
        "start_time = perf_counter()\n",
        "\n",
        "train_data_loader = common_utils.DeepSupervisionDataLoader(dataset_dir=os.path.join(args.data_path, 'train_subdivided'), \n",
        "                                                                   listfile=os.path.join(args.data_path, 'train_listfile.csv'),\n",
        "                                                                   small_part=args.small_part)\n",
        "timer1 = perf_counter()\n",
        "val_data_loader = common_utils.DeepSupervisionDataLoader(dataset_dir=os.path.join(args.data_path, 'train_subdivided'), \n",
        "                                                                 listfile=os.path.join(args.data_path, 'val_listfile.csv'),\n",
        "                                                                 small_part=args.small_part)\n",
        "timer2 = perf_counter()\n",
        "\n",
        "test_data_loader = common_utils.DeepSupervisionDataLoader(dataset_dir=os.path.join(args.data_path, 'test'), \n",
        "                                                          listfile=os.path.join(args.data_path, 'test_listfile.csv'), \n",
        "                                                          small_part=args.small_part)\n",
        "end_data_loads = perf_counter()\n",
        "\n",
        "print(\"Time to load train data: \" + str(dt.timedelta(seconds = timer1 - start_time)))\n",
        "print(\"Time to load validation data: \" + str(dt.timedelta(seconds = timer2 - timer1))) #time to generate the val data loader\n",
        "print(\"Time to load test data: \" + str(dt.timedelta(seconds = end_data_loads - timer2)))\n",
        "\n",
        "print(\"Size of training set: \" + str(len(train_data_loader._data[\"X\"])))\n",
        "print(\"Size of validation set: \" + str(len(val_data_loader._data[\"X\"])))\n",
        "print(\"Size of test set: \" + str(len(test_data_loader._data[\"X\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxIDvue03g5-",
        "outputId": "080fb5b7-7222-4dd6-e00a-6d0f6038768c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of ICU stays: 1390\n",
            "Total number of patients: 1128\n",
            "--- max number of stays per patient: 16\n",
            "--- max length of stay: 1031.0\n",
            "--- min number of stays per patient: 1\n",
            "--- min length of stay: 5.0\n",
            "--- average number of stays per patient: 1.2322695035460993\n",
            "--- average length of stay: 72.2158273381295\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "my_subject_ids = []\n",
        "my_length_of_stays = []\n",
        "test_labels = []\n",
        "val_labels = []\n",
        "train_labels = []\n",
        "\n",
        "for i in range(len(test_data_loader._data[\"X\"])):\n",
        "    my_subject_ids.append(test_data_loader._data[\"name\"][i].split('_')[0])\n",
        "    my_length_of_stays.append(test_data_loader._data[\"ts\"][i][-1])\n",
        "    test_labels.append(test_data_loader._data[\"ys\"][i])\n",
        "for i in range(len(val_data_loader._data[\"X\"])):\n",
        "    my_subject_ids.append(val_data_loader._data[\"name\"][i].split('_')[0])\n",
        "    my_length_of_stays.append(val_data_loader._data[\"ts\"][i][-1])\n",
        "    val_labels.append(val_data_loader._data[\"ys\"][i])\n",
        "for i in range(len(train_data_loader._data[\"X\"])):\n",
        "    my_subject_ids.append(train_data_loader._data[\"name\"][i].split('_')[0])\n",
        "    my_length_of_stays.append(train_data_loader._data[\"ts\"][i][-1])\n",
        "    train_labels.append(train_data_loader._data[\"ys\"][i])\n",
        "\n",
        "print(\"Total number of ICU stays: \" + str(len(my_subject_ids)))\n",
        "print(\"Total number of patients: \" + str(len(list(set(my_subject_ids)))))\n",
        "print(\"--- max number of stays per patient: \" + str(max(Counter(my_subject_ids).values())))\n",
        "print(\"--- max length of stay: \" + str(max(my_length_of_stays)))\n",
        "#print(\"--- max length of stay in days: \" + str(max(my_length_of_stays)/24))\n",
        "print(\"--- min number of stays per patient: \" + str(min(Counter(my_subject_ids).values())))\n",
        "print(\"--- min length of stay: \" + str(min(my_length_of_stays)))\n",
        "print(\"--- average number of stays per patient: \" + str(sum(Counter(my_subject_ids).values()) / len(Counter(my_subject_ids))))\n",
        "print(\"--- average length of stay: \" + str(sum(my_length_of_stays) / len(my_length_of_stays)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Breakdowns of train/test/val\n",
        "total_test_visits = 0\n",
        "total_pos_test_visits = 0\n",
        "total_val_visits = 0\n",
        "total_pos_val_visits = 0\n",
        "total_train_visits = 0\n",
        "total_pos_train_visits = 0\n",
        "\n",
        "for i in range(len(test_labels)):\n",
        "    test_labels[i] = [int(x) for x in test_labels[i]]\n",
        "    total_test_visits += len(test_labels[i])\n",
        "    total_pos_test_visits += sum(test_labels[i])\n",
        "for i in range(len(val_labels)):\n",
        "    val_labels[i] = [int(x) for x in val_labels[i]]\n",
        "    total_val_visits += len(val_labels[i])\n",
        "    total_pos_val_visits += sum(val_labels[i])\n",
        "for i in range(len(train_labels)):\n",
        "    train_labels[i] = [int(x) for x in train_labels[i]]\n",
        "    total_train_visits += len(train_labels[i])\n",
        "    total_pos_train_visits += sum(train_labels[i])\n",
        "\n",
        "total_visits = total_test_visits + total_val_visits + total_train_visits\n",
        "total_pos_visits = total_pos_test_visits + total_pos_val_visits + total_pos_train_visits\n",
        "\n",
        "print(\"Total number of visits (loaded subset): \" + str(total_visits) + \", Positive samples: \" + str(total_pos_visits))\n",
        " \n",
        "print(\"--Train--\")\n",
        "print(\"Number of stays: \" + str(len(train_data_loader._data[\"X\"])) + \", Number of visits: \" + str(total_train_visits) + \", Number positive visits: \" + str(total_pos_train_visits))\n",
        "print(\"--Validation--\")\n",
        "print(\"Number of stays: \" + str(len(val_data_loader._data[\"X\"])) + \", Number of visits: \" + str(total_val_visits) + \", Number positive visits: \" + str(total_pos_val_visits))\n",
        "print(\"--Test--\")\n",
        "print(\"Number of stays: \" + str(len(test_data_loader._data[\"X\"])) + \", Number of visits: \" + str(total_test_visits) + \", Number positive visits: \" + str(total_pos_test_visits))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbCpUE0XP2Jr",
        "outputId": "ad71c411-19d3-4d10-ede7-714d52d4a877"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of visits (loaded subset): 94819, Positive samples: 1946\n",
            "--Train--\n",
            "Number of stays: 1000, Number of visits: 67991, Number positive visits: 1607\n",
            "--Validation--\n",
            "Number of stays: 112, Number of visits: 7986, Number positive visits: 44\n",
            "--Test--\n",
            "Number of stays: 278, Number of visits: 18842, Number positive visits: 295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FXQI3QKXP359"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Methodology Explanation and Examples"
      ],
      "metadata": {
        "id": "Hy7OY20DP5Lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this study we run the following experiments:\n",
        "reproducing StageNet model\n",
        "\n",
        "1.   reproducing StageNet model\n",
        "  * evaluating against full test set\n",
        "  * evaluating against test subset\n",
        "2.   reproducing StageNet-I model\n",
        "  * evaluating against full test set\n",
        "  * evaluating against test subset\n",
        "3.   reproducing StageNet-II model\n",
        "  * evaluating against full test set\n",
        "  * evaluating against test subset\n",
        "\n",
        "Below we provide code for experiement 1b, where we train a StageNet model and then evaluate against the test subset we loaded earlier. Note that results won't exactly match those reported in the Results section because we are only using n = 1000 samples and training over 5 epochs for illustrative purposes. The earlier arguments could be modified to recreate the results:\n",
        "\n",
        "```\n",
        "args.epochs = 50\n",
        "args.small_part = 5000\n",
        "```\n"
      ],
      "metadata": {
        "id": "fK-QiztyP-Xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Sample code"
      ],
      "metadata": {
        "id": "vdVa78dwRePW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p33aDMutkLBr"
      },
      "source": [
        "Instantiate Discretizer and Normalizer objects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qEHrByLNr1VO"
      },
      "outputs": [],
      "source": [
        "discretizer = Discretizer(timestep=1.0, store_masks=True,\n",
        "                                impute_strategy='previous', start_time='zero')\n",
        "\n",
        "discretizer_header = discretizer.transform(train_data_loader._data[\"X\"][0])[1].split(',')\n",
        "cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]\n",
        "\n",
        "normalizer = Normalizer(fields=cont_channels)\n",
        "normalizer_state = 'decomp_normalizer'\n",
        "normalizer_state = os.path.join(os.path.dirname(args.data_path), normalizer_state)\n",
        "normalizer.load_params(normalizer_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD_EEz6vkQAD"
      },
      "source": [
        "Load the Batched Generators:\n",
        "\n",
        "On local machine, this took about 7 minutes for train and 1 minute for validation.\n",
        "On Google Colab, should only take 30 seconds for small_part = 5000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCtETkKqr56f",
        "outputId": "b7ebf984-0fae-4aa5-bf77-1e43bc006058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing Batched Data Loaders\n",
            "Time to load train generator: 0:00:05.019320\n",
            "Time to load validation generator: 0:00:00.578053\n"
          ]
        }
      ],
      "source": [
        "print(\"Preparing Batched Data Loaders\")\n",
        "\n",
        "start_time = perf_counter()\n",
        "train_data_gen = utils.BatchGenDeepSupervision(train_data_loader, \n",
        "                                               discretizer, normalizer, \n",
        "                                               args.batch_size, \n",
        "                                               shuffle=True, \n",
        "                                               return_names=True)\n",
        "timer1 = perf_counter()\n",
        "val_data_gen = utils.BatchGenDeepSupervision(val_data_loader, \n",
        "                                             discretizer, \n",
        "                                             normalizer, \n",
        "                                             args.batch_size, \n",
        "                                             shuffle=False, \n",
        "                                             return_names=True)\n",
        "end_time = perf_counter()\n",
        "        \n",
        "print(\"Time to load train generator: \" + str(dt.timedelta(seconds = timer1 - start_time))) #time to generate the train data gen\n",
        "print(\"Time to load validation generator: \" + str(dt.timedelta(seconds = end_time - timer1))) #time to generate the val data gen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wco6RxZcqn37"
      },
      "source": [
        "Model Construction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtIIBzbsqqi4",
        "outputId": "07bcb058-eba6-4a65-e5a2-ce962e962fb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constructing model ... \n",
            "available device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "print('Constructing model ... ')\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() == True else 'cpu')\n",
        "print(\"available device: {}\".format(device))\n",
        "\n",
        "model = StageNet(args.input_dim+17, args.rnn_dim, args.K, args.output_dim, args.chunk_level, args.dropconnect_rate, args.dropout_rate, args.dropres_rate).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fg5jV_Zqu8u"
      },
      "source": [
        "Training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekt-yLSVqwAd",
        "outputId": "c0963264-706f-42ca-fc5e-4b46e99fe493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training ... \n",
            "Chunk 0, Batch 0: Loss = 89.4580\n",
            "Epoch training time: 0:00:21.096546\n",
            "\n",
            "==>Predicting on validation\n",
            "Valid loss = 9.1935\n",
            "\n",
            "\n",
            "confusion matrix:\n",
            "[[7672    0]\n",
            " [  44    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/UIUC/CS598_DL4H/Colab_Notebooks/JR_StageNet/StageNet/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
            "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy = 0.9942975640296936\n",
            "precision class 0 = 0.9942975640296936\n",
            "precision class 1 = nan\n",
            "recall class 0 = 1.0\n",
            "recall class 1 = 0.0\n",
            "AUC of ROC = 0.8024812778462413\n",
            "AUC of PRC = 0.013002167467328817\n",
            "min(+P, Se) = 0.018509254627313655\n",
            "\n",
            "\n",
            "------------ Save best model ------------\n",
            "\n",
            "Chunk 1, Batch 0: Loss = 27.6849\n",
            "Epoch training time: 0:00:13.767411\n",
            "\n",
            "==>Predicting on validation\n",
            "Valid loss = 6.0051\n",
            "\n",
            "\n",
            "confusion matrix:\n",
            "[[7672    0]\n",
            " [  44    0]]\n",
            "accuracy = 0.9942975640296936\n",
            "precision class 0 = 0.9942975640296936\n",
            "precision class 1 = nan\n",
            "recall class 0 = 1.0\n",
            "recall class 1 = 0.0\n",
            "AUC of ROC = 0.9570071807754289\n",
            "AUC of PRC = 0.05887201537875261\n",
            "min(+P, Se) = 0.07494145199063232\n",
            "\n",
            "\n",
            "------------ Save best model ------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/UIUC/CS598_DL4H/Colab_Notebooks/JR_StageNet/StageNet/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
            "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 2, Batch 0: Loss = 21.5394\n",
            "Epoch training time: 0:00:13.991198\n",
            "\n",
            "==>Predicting on validation\n",
            "Valid loss = 5.0819\n",
            "\n",
            "\n",
            "confusion matrix:\n",
            "[[7669    3]\n",
            " [  44    0]]\n",
            "accuracy = 0.9939087629318237\n",
            "precision class 0 = 0.9942953586578369\n",
            "precision class 1 = 0.0\n",
            "recall class 0 = 0.9996089935302734\n",
            "recall class 1 = 0.0\n",
            "AUC of ROC = 0.9691173333965304\n",
            "AUC of PRC = 0.0787611277519728\n",
            "min(+P, Se) = 0.09803921568627451\n",
            "\n",
            "\n",
            "------------ Save best model ------------\n",
            "\n",
            "Chunk 3, Batch 0: Loss = 22.7054\n",
            "Epoch training time: 0:00:13.363892\n",
            "\n",
            "==>Predicting on validation\n",
            "Valid loss = 4.0893\n",
            "\n",
            "\n",
            "confusion matrix:\n",
            "[[7653   19]\n",
            " [  44    0]]\n",
            "accuracy = 0.9918351769447327\n",
            "precision class 0 = 0.9942834973335266\n",
            "precision class 1 = 0.0\n",
            "recall class 0 = 0.9975234866142273\n",
            "recall class 1 = 0.0\n",
            "AUC of ROC = 0.9735105223243911\n",
            "AUC of PRC = 0.09709196786107609\n",
            "min(+P, Se) = 0.14166666666666666\n",
            "\n",
            "\n",
            "------------ Save best model ------------\n",
            "\n",
            "Chunk 4, Batch 0: Loss = 20.0406\n",
            "Epoch training time: 0:00:13.964875\n",
            "\n",
            "==>Predicting on validation\n",
            "Valid loss = 4.0748\n",
            "\n",
            "\n",
            "confusion matrix:\n",
            "[[7617   55]\n",
            " [  27   17]]\n",
            "accuracy = 0.989372730255127\n",
            "precision class 0 = 0.9964678287506104\n",
            "precision class 1 = 0.2361111044883728\n",
            "recall class 0 = 0.9928310513496399\n",
            "recall class 1 = 0.3863636255264282\n",
            "AUC of ROC = 0.9719019575315195\n",
            "AUC of PRC = 0.13341935787044115\n",
            "min(+P, Se) = 0.25\n",
            "\n",
            "\n",
            "------------ Save best model ------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Start training ... ')\n",
        "\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "batch_loss = []\n",
        "max_auprc = 0\n",
        "val_auroc = []\n",
        "val_auprc = []\n",
        "acc = []\n",
        "prec0 = []\n",
        "prec1 = []\n",
        "rec0 = []\n",
        "rec1 = []\n",
        "minpse = []\n",
        "\n",
        "my_epoch_times = []\n",
        "\n",
        "file_name = './saved_weights/' + args.file_name + datetime.now().strftime(\"%d%m%Y_%H_%M\")\n",
        "for each_chunk in range(args.epochs):\n",
        "    epoch_starttime = perf_counter()\n",
        "    cur_batch_loss = []\n",
        "    model.train()\n",
        "    for each_batch in range(train_data_gen.steps):\n",
        "        batch_data = next(train_data_gen)\n",
        "        batch_name = batch_data['names']\n",
        "        batch_data = batch_data['data']\n",
        "\n",
        "        batch_x = torch.tensor(batch_data[0][0], dtype=torch.float32).to(device)\n",
        "        batch_mask = torch.tensor(batch_data[0][1], dtype=torch.float32).unsqueeze(-1).to(device)\n",
        "        batch_y = torch.tensor(batch_data[1], dtype=torch.float32).to(device)\n",
        "        tmp = torch.zeros(batch_x.size(0),17, dtype=torch.float32).to(device)\n",
        "        batch_interval = torch.zeros((batch_x.size(0),batch_x.size(1),17), dtype=torch.float32).to(device)\n",
        "        \n",
        "        for i in range(batch_x.size(1)):\n",
        "            cur_ind = batch_x[:,i,-17:]\n",
        "            tmp+=(cur_ind == 0).float()\n",
        "            batch_interval[:, i, :] = cur_ind * tmp\n",
        "            tmp[cur_ind==1] = 0        \n",
        "        \n",
        "        if batch_mask.size()[1] > 400:\n",
        "            batch_x = batch_x[:, :400, :]\n",
        "            batch_mask = batch_mask[:, :400, :]\n",
        "            batch_y = batch_y[:, :400, :]\n",
        "            batch_interval = batch_interval[:, :400, :]\n",
        "\n",
        "        batch_x = torch.cat((batch_x, batch_interval), dim=-1)\n",
        "        batch_time = torch.ones((batch_x.size(0), batch_x.size(1)), dtype=torch.float32).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        cur_output, _ = model(batch_x, batch_time, device)\n",
        "        masked_output = cur_output * batch_mask \n",
        "        loss = batch_y * torch.log(masked_output + 1e-7) + (1 - batch_y) * torch.log(1 - masked_output + 1e-7)\n",
        "        loss = torch.sum(loss, dim=1) / torch.sum(batch_mask, dim=1)\n",
        "        loss = torch.neg(torch.sum(loss))\n",
        "        cur_batch_loss.append(loss.cpu().detach().numpy())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if each_batch % 50 == 0:\n",
        "            print('Chunk %d, Batch %d: Loss = %.4f'%(each_chunk, each_batch, cur_batch_loss[-1]))\n",
        "\n",
        "    batch_loss.append(cur_batch_loss)\n",
        "    train_loss.append(np.mean(np.array(cur_batch_loss)))\n",
        "    \n",
        "    epoch_endtime = perf_counter()\n",
        "    my_epoch_times.append(epoch_endtime - epoch_starttime)\n",
        "\n",
        "    print('Epoch training time: ' + str(dt.timedelta(seconds = epoch_endtime - epoch_starttime)))\n",
        "    \n",
        "    print(\"\\n==>Predicting on validation\")\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        cur_val_loss = []\n",
        "        valid_true = []\n",
        "        valid_pred = []\n",
        "        for each_batch in range(val_data_gen.steps):\n",
        "            valid_data = next(val_data_gen)\n",
        "            valid_name = valid_data['names']\n",
        "            valid_data = valid_data['data']\n",
        "            \n",
        "            valid_x = torch.tensor(valid_data[0][0], dtype=torch.float32).to(device)\n",
        "            valid_mask = torch.tensor(valid_data[0][1], dtype=torch.float32).unsqueeze(-1).to(device)\n",
        "            valid_y = torch.tensor(valid_data[1], dtype=torch.float32).to(device)\n",
        "            tmp = torch.zeros(valid_x.size(0),17, dtype=torch.float32).to(device)\n",
        "            valid_interval = torch.zeros((valid_x.size(0),valid_x.size(1),17), dtype=torch.float32).to(device)\n",
        "            \n",
        "            for i in range(valid_x.size(1)):\n",
        "                cur_ind = valid_x[:,i,-17:]\n",
        "                tmp+=(cur_ind == 0).float()\n",
        "                valid_interval[:, i, :] = cur_ind * tmp\n",
        "                tmp[cur_ind==1] = 0  \n",
        "            \n",
        "            if valid_mask.size()[1] > 400:\n",
        "                valid_x = valid_x[:, :400, :]\n",
        "                valid_mask = valid_mask[:, :400, :]\n",
        "                valid_y = valid_y[:, :400, :]\n",
        "                valid_interval = valid_interval[:, :400, :]\n",
        "            \n",
        "            valid_x = torch.cat((valid_x, valid_interval), dim=-1)\n",
        "            valid_time = torch.ones((valid_x.size(0), valid_x.size(1)), dtype=torch.float32).to(device)\n",
        "            \n",
        "            valid_output, valid_dis = model(valid_x, valid_time, device)\n",
        "            masked_valid_output = valid_output * valid_mask\n",
        "\n",
        "            valid_loss = valid_y * torch.log(masked_valid_output + 1e-7) + (1 - valid_y) * torch.log(1 - masked_valid_output + 1e-7)\n",
        "            valid_loss = torch.sum(valid_loss, dim=1) / torch.sum(valid_mask, dim=1)\n",
        "            valid_loss = torch.neg(torch.sum(valid_loss))\n",
        "            cur_val_loss.append(valid_loss.cpu().detach().numpy())\n",
        "\n",
        "            for m, t, p in zip(valid_mask.cpu().numpy().flatten(), valid_y.cpu().numpy().flatten(), valid_output.cpu().detach().numpy().flatten()):\n",
        "                if np.equal(m, 1):\n",
        "                    valid_true.append(t)\n",
        "                    valid_pred.append(p)\n",
        "\n",
        "        val_loss.append(np.mean(np.array(cur_val_loss)))\n",
        "        print('Valid loss = %.4f'%(val_loss[-1]))\n",
        "        print('\\n')\n",
        "        valid_pred = np.array(valid_pred)\n",
        "        valid_pred = np.stack([1 - valid_pred, valid_pred], axis=1)\n",
        "        ret = metrics.print_metrics_binary(valid_true, valid_pred)\n",
        "        print()\n",
        "\n",
        "        cur_auprc = ret['auprc']\n",
        "\n",
        "        val_auprc.append(cur_auprc)\n",
        "        val_auroc.append(ret['auroc'])\n",
        "        acc.append(ret['acc'])\n",
        "        prec0.append(ret['prec0'])\n",
        "        prec1.append(ret['prec1'])\n",
        "        rec0.append(ret['rec0'])\n",
        "        rec1.append(ret['rec1'])\n",
        "        minpse.append(ret['minpse'])\n",
        "        \n",
        "        if cur_auprc > max_auprc:\n",
        "            max_auprc = cur_auprc\n",
        "            state = {\n",
        "                'net': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'chunk': each_chunk\n",
        "            }\n",
        "            torch.save(state, file_name)\n",
        "            print('\\n------------ Save best model ------------\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQJ3U4Pt2Z1m"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAhACEgF2cR5"
      },
      "source": [
        "Evaluate on Test Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wydRmuIv2p6f"
      },
      "source": [
        "Load last checkpoint:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ligyykD-2kSO",
        "outputId": "53eac296-bdb8-4a67-b7da-87321a9c0283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "last saved model is in chunk 4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StageNet(\n",
              "  (kernel): Linear(in_features=94, out_features=1542, bias=True)\n",
              "  (recurrent_kernel): Linear(in_features=385, out_features=1542, bias=True)\n",
              "  (nn_scale): Linear(in_features=384, out_features=64, bias=True)\n",
              "  (nn_rescale): Linear(in_features=64, out_features=384, bias=True)\n",
              "  (nn_conv): Conv1d(384, 384, kernel_size=(10,), stride=(1,))\n",
              "  (nn_output): Linear(in_features=384, out_features=1, bias=True)\n",
              "  (nn_dropconnect): Dropout(p=0.5, inplace=False)\n",
              "  (nn_dropconnect_r): Dropout(p=0.5, inplace=False)\n",
              "  (nn_dropout): Dropout(p=0.5, inplace=False)\n",
              "  (nn_dropres): Dropout(p=0.3, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "checkpoint = torch.load(file_name)\n",
        "save_chunk = checkpoint['chunk']\n",
        "print(\"last saved model is in chunk {}\".format(save_chunk))\n",
        "model.load_state_dict(checkpoint['net'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3Kf4CtO2tFB"
      },
      "source": [
        "Load test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kpIYWig2v3u",
        "outputId": "95d9be90-1a67-4ba8-b2fe-97ff1fcddebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time to load test generator: 0:00:01.348980\n"
          ]
        }
      ],
      "source": [
        "start_data_loads = perf_counter()\n",
        "test_data_gen = utils.BatchGenDeepSupervision(test_data_loader, discretizer, normalizer, args.batch_size, shuffle=False, return_names=True)\n",
        "end_data_loads = perf_counter()\n",
        "\n",
        "print(\"Time to load test generator: \" + str(dt.timedelta(seconds = end_data_loads - start_data_loads)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L89l13k73iwu"
      },
      "source": [
        "Test the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS8wo5A33mci",
        "outputId": "ebabb0df-de18-4f89-c3c2-5aa2a7a9e3d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing model ... \n",
            "Checkpoint to be loaded: \n",
            "./saved_weights/trained_model09052023_02_16\n",
            "last saved model is in chunk 4\n",
            "Generating data...\n",
            "Time to load test data: 0:00:02.942288\n",
            "Time to load test generator: 0:00:01.339803\n",
            "Size of test set: 278\n",
            "Test loss = 7.5791\n",
            "\n",
            "\n",
            "confusion matrix:\n",
            "[[18307    85]\n",
            " [  275    20]]\n",
            "accuracy = 0.9807352423667908\n",
            "precision class 0 = 0.9852007031440735\n",
            "precision class 1 = 0.190476194024086\n",
            "recall class 0 = 0.9953784346580505\n",
            "recall class 1 = 0.06779661029577255\n",
            "AUC of ROC = 0.7428179901357259\n",
            "AUC of PRC = 0.09318913646989999\n",
            "min(+P, Se) = 0.1282051282051282\n"
          ]
        }
      ],
      "source": [
        "print('Testing model ... ')\n",
        "print('Checkpoint to be loaded: ')\n",
        "print(file_name)\n",
        "\n",
        "checkpoint = torch.load(file_name)\n",
        "save_chunk = checkpoint['chunk']\n",
        "print(\"last saved model is in chunk {}\".format(save_chunk))\n",
        "model.load_state_dict(checkpoint['net'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "model.eval()\n",
        "\n",
        "start_time = perf_counter()\n",
        "#test_data_loader = common_utils.DeepSupervisionDataLoader(dataset_dir=os.path.join(args.data_path, 'test'),\n",
        "#                                                                listfile=os.path.join(args.data_path, 'test_listfile.csv'), small_part=args.small_part)\n",
        "\n",
        "test_data_loader = common_utils.DeepSupervisionDataLoader(dataset_dir=os.path.join(args.data_path, 'test'),\n",
        "                                                          listfile=os.path.join(args.data_path, 'test_listfile.csv'),\n",
        "                                                          small_part=args.small_part)\n",
        "\n",
        "\n",
        "timer1 = perf_counter()\n",
        "test_data_gen = utils.BatchGenDeepSupervision(test_data_loader, discretizer,\n",
        "                                            normalizer, args.batch_size,\n",
        "                                            shuffle=False, return_names=True)\n",
        "end_time = perf_counter()\n",
        "\n",
        "print(\"Time to load test data: \" + str(dt.timedelta(seconds = timer1 - start_time)))\n",
        "print(\"Time to load test generator: \" + str(dt.timedelta(seconds = end_time - timer1)))\n",
        "print(\"Size of test set: \" + str(len(test_data_loader._data[\"X\"])))\n",
        "\n",
        "with torch.no_grad():\n",
        "    torch.manual_seed(RANDOM_SEED)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "    cur_test_loss = []\n",
        "    test_true = []\n",
        "    test_pred = []\n",
        "    \n",
        "    for each_batch in range(test_data_gen.steps):\n",
        "        test_data = next(test_data_gen)\n",
        "        test_name = test_data['names']\n",
        "        test_data = test_data['data']\n",
        "\n",
        "        test_x = torch.tensor(test_data[0][0], dtype=torch.float32).to(device)\n",
        "        test_mask = torch.tensor(test_data[0][1], dtype=torch.float32).unsqueeze(-1).to(device)\n",
        "        test_y = torch.tensor(test_data[1], dtype=torch.float32).to(device)\n",
        "        tmp = torch.zeros(test_x.size(0),17, dtype=torch.float32).to(device)\n",
        "        test_interval = torch.zeros((test_x.size(0),test_x.size(1),17), dtype=torch.float32).to(device)\n",
        "\n",
        "        for i in range(test_x.size(1)):\n",
        "            cur_ind = test_x[:,i,-17:]\n",
        "            tmp+=(cur_ind == 0).float()\n",
        "            test_interval[:, i, :] = cur_ind * tmp\n",
        "            tmp[cur_ind==1] = 0  \n",
        "        \n",
        "        if test_mask.size()[1] > 400:\n",
        "            test_x = test_x[:, :400, :]\n",
        "            test_mask = test_mask[:, :400, :]\n",
        "            test_y = test_y[:, :400, :]\n",
        "            test_interval = test_interval[:, :400, :]\n",
        "        \n",
        "        test_x = torch.cat((test_x, test_interval), dim=-1)\n",
        "        test_time = torch.ones((test_x.size(0), test_x.size(1)), dtype=torch.float32).to(device)\n",
        "        \n",
        "        test_output, test_dis = model(test_x, test_time, device)\n",
        "        masked_test_output = test_output * test_mask\n",
        "\n",
        "        test_loss = test_y * torch.log(masked_test_output + 1e-7) + (1 - test_y) * torch.log(1 - masked_test_output + 1e-7)\n",
        "        test_loss = torch.sum(test_loss, dim=1) / torch.sum(test_mask, dim=1)\n",
        "        test_loss = torch.neg(torch.sum(test_loss))\n",
        "        cur_test_loss.append(test_loss.cpu().detach().numpy()) \n",
        "        \n",
        "        for m, t, p in zip(test_mask.cpu().numpy().flatten(), test_y.cpu().numpy().flatten(), test_output.cpu().detach().numpy().flatten()):\n",
        "            if np.equal(m, 1):\n",
        "                test_true.append(t)\n",
        "                test_pred.append(p)\n",
        "    \n",
        "    print('Test loss = %.4f'%(np.mean(np.array(cur_test_loss))))\n",
        "    print('\\n')\n",
        "    test_pred = np.array(test_pred)\n",
        "    test_pred = np.stack([1 - test_pred, test_pred], axis=1)\n",
        "    test_ret = metrics.print_metrics_binary(test_true, test_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "hWT_FioPTfbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results"
      ],
      "metadata": {
        "id": "P_1YkNX2TkbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we translate the results for the sample experiment from this notebook:"
      ],
      "metadata": {
        "id": "o3hiH6YhTpAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| AUPRC  | AUROC  | min(Re, P+) |\n",
        "|--------|--------|-------------|\n",
        "| 0.0932 | 0.7428 | 0.1282      |"
      ],
      "metadata": {
        "id": "f2iH3Q5VVWdD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we provide the results from all experiements. We also show results from 2 baseline models used in the original paper, and we show all results reported from the original paper in comparison to our own experiment results:"
      ],
      "metadata": {
        "id": "_kstGrPwU1NP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|                        |          | AUPRC |        |          | AUROC |        |          | min(Re,P+) |        |\n",
        "|------------------------|:--------:|-------|--------|:--------:|-------|--------|:--------:|------------|--------|\n",
        "|                        | Original | Full  | Subset | Original | Full  | Subset | Original | Full       | Subset |\n",
        "| Baseline1: ON-LSTM     | 0.304    | ---   | ---    | 0.895    | ---   | ---    | 0.343    | ---        | ---    |\n",
        "| Baseline2: Health-LSTM | 0.291    | ---   | ---    | 0.897    | ---   | ---    | 0.325    | ---        | ---    |\n",
        "| Pre-Trained StageNet   | 0.323    | 0.341 | 0.289  | 0.903    | 0.909 | 0.890  | 0.372    | 0.390      | 0.347  |\n",
        "| Reproduced StageNet    | 0.323    | 0.228 | 0.206  | 0.903    | 0.874 | 0.842  | 0.372    | 0.292      | 0.280  |\n",
        "| Ablation1: StageNet-I  | 0.313    | 0.226 | 0.209  | 0.899    | 0.850 | 0.838  | 0.360    | 0.315      | 0.279  |\n",
        "| Ablation2: StageNet-II | 0.311    | 0.220 | 0.211  | 0.897    | 0.872 | 0.844  | 0.358    | 0.287      | 0.280  |"
      ],
      "metadata": {
        "id": "aoYk78xRVtsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WTIS0hJ3ZTFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References"
      ],
      "metadata": {
        "id": "K10Kmt4aV4uB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Junyi Gao, Cao Xiao, Yasha Wang, Wen Tang, Lucas M.\n",
        "Glass, and Jimeng Sun. 2020. Stagenet: Stage-aware\n",
        "neural networks for health risk prediction. CoRR,\n",
        "abs/2001.10054.\n",
        "\n",
        "Amaral L. Glass L. Hausdorff J. Ivanov P. C. Mark R.\n",
        "... Stanley H. E. Goldberger, A. 2000. Physiobank,\n",
        "physiotoolkit, and physionet: Components of a new\n",
        "research resource for complex physiologic signals.\n",
        "PhysioNet, 101:e215–e220.\n",
        "\n",
        "Hrayr Harutyunyan, Hrant Khachatrian, David C. Kale,\n",
        "Greg Ver Steeg, and Aram Galstyan. 2019. Multitask\n",
        "learning and benchmarking with clinical time series\n",
        "data. Scientific Data, 6(1):96.\n",
        "\n",
        "Nasir Hayat, Krzysztof J. Geras, and Farah E. Shamout.\n",
        "2022. Medfuse: Multi-modal fusion with clinical\n",
        "time-series data and chest x-ray images.\n",
        "\n",
        "Xi Zhang Fei Wang Anil K Jain Inci M Baytas,\n",
        "Cao Xiao and Jiayu Zhou. 2017. Patient subtyp-\n",
        "ing via time-aware lstm networks. Proceedings of\n",
        "the 23rd ACM SIGKDD international conference on\n",
        "knowledge discovery and data mining, pages 65–74.\n",
        "\n",
        "Pollard Tom J. Shen Lu Lehman Li-wei H. Feng\n",
        "Mengling Ghassemi Mohammad Moody Benjamin\n",
        "Szolovits Peter Anthony Celi Leo Mark Roger G.\n",
        "Johnson, Alistair E.W. 2016. Mimic-iii, a freely ac-\n",
        "cessible critical care database.\n",
        "\n",
        "Tengfei Ma, Cao Xiao, and Fei Wang. Health-ATM:\n",
        "A Deep Architecture for Multifaceted Patient Health\n",
        "Record Representation and Risk Prediction, pages\n",
        "261–269.\n",
        "\n",
        "Yikang Shen, Shawn Tan, Alessandro Sordoni, and\n",
        "Aaron C. Courville. 2018. Ordered neurons: Inte-\n",
        "grating tree structures into recurrent neural networks.\n",
        "CoRR, abs/1810.09536.\n"
      ],
      "metadata": {
        "id": "u_OZJ5RJV_Dk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Thank you!"
      ],
      "metadata": {
        "id": "84TP-a59WVSJ"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1b3W_rpWA8-ZkyaRwUwWMKXtE9GVNubr4",
      "authorship_tag": "ABX9TyMkFBPL/EiyDw8foa7xs4RP",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
